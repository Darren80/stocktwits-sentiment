{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jWq6edktVvVB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets\n",
    "!pip install wandb\n",
    "!pip install torch\n",
    "!pip install Cython\n",
    "\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install -U transformers\n",
    "!pip install peft\n",
    "!pip install -U bitsandbytes\n",
    "!pip install tensorboard\n",
    "!pip install accelerate -U\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "!pip install paramiko scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6YnI8mFP9tiM"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6478a1cae3f9400d89d078300c0a3c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "base_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"dohonba/mistral_7b_fingpt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_prompt = \"\"\"\n",
    "# Is this sentence self-promotional? Answer with {no/yes}? \"Building brick by brick, our analysts motto! Pay a visit to our Community\".\n",
    "# \"\"\"\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "#     decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "# print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhUFh_uaXG14",
    "outputId": "526a32e8-6772-4981-a08b-9fa98cee7de0"
   },
   "outputs": [],
   "source": [
    "# Function to classify emotion of a sentence\n",
    "def classify_sentiment(sentence):\n",
    "    eval_prompt = f\"\"\"Context: {sentence}\n",
    "\n",
    "Question: 'What is the sentiment of this sentence? Please choose an answer from {{strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}}.'\n",
    "\"\"\"\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "        decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    answer = decoded_output.split(\"Answer: \", 1)[1]\n",
    "    return answer  # You might need to further process this to extract the emotion\n",
    "\n",
    "def classify_emotion(sentence):\n",
    "    eval_prompt = f\"\"\"Context: {sentence}\n",
    "\n",
    "Question: 'What is the emotion shown in this text? Please choose an answer from {{anger/fear/joy/love/sadness/surprise/neutral}}'.\n",
    "\"\"\"\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "        decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    answer = decoded_output.split(\"Answer: \", 1)[1]\n",
    "    return answer  # You might need to further process this to extract the emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wp5o1ZXYXVy6"
   },
   "outputs": [],
   "source": [
    "# classify_emotion(\"I love it. Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "from scp import SCPClient\n",
    "\n",
    "def create_ssh_client(server, port, user, password):\n",
    "    client = paramiko.SSHClient()\n",
    "    client.load_system_host_keys()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    client.connect(server, port, user, password)\n",
    "    return client\n",
    "\n",
    "def upload_files(ssh_client, local_path, remote_path):\n",
    "    with SCPClient(ssh_client.get_transport()) as scp:\n",
    "        scp.put(local_path, remote_path)  # Use put for uploading\n",
    "\n",
    "def append_file(ssh_client, local_path, remote_path, temp_path):\n",
    "    # Step 1: Transfer the file to a temporary location\n",
    "    with SCPClient(ssh_client.get_transport()) as scp:\n",
    "        scp.put(local_path, temp_path)\n",
    "    \n",
    "    # Step 2: Append the content of the temporary file to the target file\n",
    "    command = f'cat {temp_path} >> {remote_path}; rm {temp_path}'\n",
    "    ssh_client.exec_command(command)\n",
    "\n",
    "def download_files(ssh_client, remote_path, local_path):\n",
    "    with SCPClient(ssh_client.get_transport()) as scp:\n",
    "        scp.get(remote_path, local_path)\n",
    "\n",
    "# Optional: Execute a command or run a script on the remote machine\n",
    "# stdin, stdout, stderr = ssh_client.exec_command('python /path/to/remote/script.py')\n",
    "# print(stdout.read().decode())  # Assuming the script has output\n",
    "\n",
    "def close_client():\n",
    "    # Close the SSH connection\n",
    "    ssh_client.close()\n",
    "\n",
    "# # SSH Connection Info\n",
    "# server = 'sshhop.hopto.org'\n",
    "# port = 22  # Default SSH port\n",
    "# user = 'bow33'\n",
    "# password = 'wee.com123'\n",
    "# remote_path = '/C:/Users/bow33/Documents/GitHub/stocktwits-sentiment/processed_english_tweets3.jsonl'\n",
    "# local_path = './processed_english_tweets.jsonl'\n",
    "\n",
    "# # Create SSH client and connect\n",
    "# ssh_client = create_ssh_client(server, port, user, password)\n",
    "# upload_files(ssh_client, './processed_english_tweets.jsonl', '/C:/Users/bow33/Documents/GitHub/stocktwits-sentiment/processed_english_tweets.jsonl')\n",
    "# download_files(ssh_client, '/C:/Users/bow33/Documents/GitHub/stocktwits-sentiment/english_tweets.jsonl', './english_tweets.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Tweets:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  10%|█         | 1/10 [00:03<00:29,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  20%|██        | 2/10 [00:06<00:25,  3.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  30%|███       | 3/10 [00:10<00:23,  3.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  40%|████      | 4/10 [00:13<00:20,  3.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  50%|█████     | 5/10 [00:17<00:17,  3.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  60%|██████    | 6/10 [00:20<00:14,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  70%|███████   | 7/10 [00:23<00:09,  3.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  80%|████████  | 8/10 [00:26<00:06,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  90%|█████████ | 9/10 [00:30<00:03,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets: 100%|██████████| 10/10 [00:32<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0: strong negative', '1: strong positive', '2: mildly negative', '3: mildly positive', '4: mildly positive', '5: mildly negative', '6: neutral', '7: neutral', '8: mildly positive', '9: neutral']\n",
      "['0: anger', '1: love', '2: neutral', '3: neutral', '4: neutral', '5: neutral', '6: neutral', '7: neutral', '8: neutral', '9: neutral']\n",
      "0: Tesla is crap , buy German cars!\n",
      "1: damn that is one sexy.... Tesla.\n",
      "2: Well, we all hope so, , but Tesla stock price doesn't prove it.\n",
      "3: I really would like to have a Referral program like Tesla. I’ve sold my 6th and I’m not even working for you. new lease program is not well known and I called three families who didn’t want to wait - said “now you can get one in 1-6 weeks”. Walked them through it all.\n",
      "4: Yes exactly but prices have corrected at least with Tesla. Buying new vs used is crazy at this time.\n",
      "5: I applaud for the heads up on a planned price change. The out-of-the blue price changes are a sales tactic that can cause a negative experience for customers who recently ordered\n",
      "6: So, are we getting X Computers next?\n",
      "7: ⁉⁉ …⁉\n",
      "8: Boy has that ever me into trouble But hey I'm optimistic\n",
      "9: Elon Musk's Tesla Roadster is currently 1.02 AU from the Sun, 0.70 AU from Earth, and 2.45 AU from Mars. For more information, see\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "\n",
    "server = 'sshhop.hopto.org'\n",
    "port = 22  # Default SSH port\n",
    "user = 'bow33'\n",
    "password = 'wee.com123'\n",
    "\n",
    "def download_recent_tweets():\n",
    "    ssh_client = create_ssh_client(server, port, user, password)\n",
    "    download_files(ssh_client, '/C:/Users/bow33/Documents/GitHub/stocktwits-sentiment/english_tweets.jsonl', './english_tweets.jsonl')\n",
    "    close_client()\n",
    "\n",
    "def upload_processed_tweets():\n",
    "    ssh_client = create_ssh_client(server, port, user, password)\n",
    "    append_file(ssh_client, './processed_english_tweets.jsonl', '/C:/Users/bow33/Documents/GitHub/stocktwits-sentiment/processed_english_tweets.jsonl', '/C:/Users/bow33/Documents/GitHub/stocktwits-sentiment/processed_english_tweets_temp.jsonl')\n",
    "    close_client()\n",
    "\n",
    "def load_recent_tweets(file_path, cutoff_time):\n",
    "    download_recent_tweets()\n",
    "    recent_tweets = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            tweet_time = datetime.fromisoformat(tweet['date'])  # Assuming UTC and removing 'Z'\n",
    "            if tweet_time > cutoff_time:\n",
    "                recent_tweets.append(tweet)  # Assuming text is under 'rawContent'\n",
    "    return recent_tweets\n",
    "\n",
    "def save_processed_tweets(file_path, tweets):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for tweet in tweets:\n",
    "            # Create a new dictionary with only the required fields\n",
    "            filtered_tweet = {\n",
    "                'id': tweet['id'],  # Assuming each tweet has a unique 'id'\n",
    "                'date': tweet['date'],\n",
    "                'cleanContent': tweet.get('cleanContent', ''),\n",
    "                'url': tweet.get('url', ''),\n",
    "                'emotion': tweet.get('emotion', ''),\n",
    "                'sentiment': tweet.get('sentiment', '')\n",
    "            }\n",
    "            json.dump(filtered_tweet, file)\n",
    "            file.write('\\n')\n",
    "    upload_processed_tweets()\n",
    "\n",
    "# Calculate the cutoff time for the last 30 minutes\n",
    "cutoff_time = datetime.utcnow().replace(tzinfo=pytz.utc) - timedelta(minutes=30)\n",
    "tweets = load_recent_tweets('./english_tweets.jsonl', cutoff_time)\n",
    "\n",
    "limit = 10\n",
    "tweets = tweets[:limit]\n",
    "\n",
    "sentiment_results = []\n",
    "emotion_results = []\n",
    "for i, tweet in enumerate(tqdm(tweets, desc=\"Analyzing Tweets\")):\n",
    "    clean_content = tweet.get('cleanContent', '')\n",
    "    \n",
    "    sentiment = classify_sentiment(clean_content)\n",
    "    emotion = classify_emotion(clean_content)\n",
    "    \n",
    "    sentiment_results.append(f\"{i}: \" + sentiment)\n",
    "    emotion_results.append(f\"{i}: \" + emotion)\n",
    "    \n",
    "    tweet['sentiment'] = sentiment\n",
    "    tweet['emotion'] = emotion\n",
    "\n",
    "#Save inference\n",
    "save_processed_tweets('./processed_english_tweets.jsonl', tweets)\n",
    "\n",
    "# Print results\n",
    "print(sentiment_results)\n",
    "print(emotion_results)\n",
    "for i, tweet in enumerate(tweets):\n",
    "    print(f\"{i}: \" + tweet.get('cleanContent', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = NewsPlease.from_url('https://finance.yahoo.com/news/alaska-airlines-begun-flying-boeing-150009733.html')\n",
    "\n",
    "# Split the article text into sentences\n",
    "sentences = sent_tokenize(article.maintext)\n",
    "print(\"Sentences in the article: \", len(sentences))\n",
    "\n",
    "# Classify emotion for each sentence with a progress bar\n",
    "emotion_results = []\n",
    "for i, sentence in enumerate(tqdm(sentences, desc=\"Processing Sentences\")):\n",
    "    emotion = classify_sentiment(sentence)\n",
    "    emotion_results.append(emotion)\n",
    "\n",
    "# Do something with the results\n",
    "print(emotion_results)\n",
    "\n",
    "# print(article.maintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
