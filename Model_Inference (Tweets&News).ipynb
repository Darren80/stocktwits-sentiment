{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jWq6edktVvVB"
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets\n",
    "!pip install wandb\n",
    "!pip install torch\n",
    "!pip install Cython\n",
    "\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install -U transformers\n",
    "!pip install peft\n",
    "!pip install -U bitsandbytes\n",
    "!pip install tensorboard\n",
    "!pip install accelerate -U\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install paramiko scp\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a Model to Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6YnI8mFP9tiM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 291 tensors from C:\\Users\\bow33\\models\\mistral_7b_v0.2_fingpt_Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7338.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3584\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   448.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    16.33 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   260.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': '.', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.bos_token_id': '1'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "def load_to_gpu():\n",
    "    base_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, \"dohonba/mistral_7b_fingpt\")\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_llamacpp():\n",
    "    from llama_cpp import Llama\n",
    "    llm = Llama(model_path=r\"C:\\Users\\bow33\\models\\mistral_7b_v0.2_fingpt_Q8_0.gguf\", n_gpu_layers=500, n_ctx=3584, n_batch=521, verbose=True)\n",
    "    return llm\n",
    "    \n",
    "# Choose a model to use\n",
    "llm = load_llamacpp()\n",
    "# model, tokenizer = load_to_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_prompt = \"\"\"\n",
    "# Is this sentence self-promotional? Answer with {no/yes}? \"Building brick by brick, our analysts motto! Pay a visit to our Community\".\n",
    "# \"\"\"\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "#     decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "# print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhUFh_uaXG14",
    "outputId": "526a32e8-6772-4981-a08b-9fa98cee7de0"
   },
   "outputs": [],
   "source": [
    "# Function to classify emotion of a sentence\n",
    "mode = \"llamacpp\"\n",
    "\n",
    "def generate(eval_prompt):\n",
    "    answer = None\n",
    "    if mode == \"gpu\":\n",
    "        model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "            decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "        answer = decoded_output.split(\"Answer: \", 1)[1]\n",
    "    elif mode == \"llamacpp\":\n",
    "        output = llm(eval_prompt, max_tokens=12, echo=True)\n",
    "        \n",
    "        answer = output['choices'][0]['text']\n",
    "        answer = answer.split(\"Answer: \", 1)[1]\n",
    "    return answer\n",
    "\n",
    "def classify_sentiment(sentence):\n",
    "    eval_prompt = f\"\"\"Context: {sentence}\n",
    "\n",
    "Question: 'What is the sentiment of this sentence? Please choose an answer from {{strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}}.'\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "    answer = generate(eval_prompt)\n",
    "    return answer  # You might need to further process this to extract the emotion\n",
    "\n",
    "def classify_emotion(sentence):\n",
    "    eval_prompt = f\"\"\"Context: {sentence}\n",
    "\n",
    "Question: 'What is the emotion shown in this text? Please choose an answer from {{anger/fear/joy/love/sadness/surprise/neutral}}'.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    answer = generate(eval_prompt)\n",
    "    return answer  # You might need to further process this to extract the emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wp5o1ZXYXVy6"
   },
   "outputs": [],
   "source": [
    "# classify_emotion(\"I love it. Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paramiko'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mparamiko\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SCPClient\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_ssh_client\u001b[39m(server, port, user, password):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'paramiko'"
     ]
    }
   ],
   "source": [
    "import paramiko\n",
    "from scp import SCPClient\n",
    "\n",
    "def create_ssh_client(server, port, user, password):\n",
    "    client = paramiko.SSHClient()\n",
    "    client.load_system_host_keys()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    client.connect(server, port, user, password, compress=True)\n",
    "    return client\n",
    "\n",
    "def upload_files(ssh_client, local_path, remote_path):\n",
    "    with SCPClient(ssh_client.get_transport()) as scp:\n",
    "        scp.put(local_path, remote_path)  # Use put for uploading\n",
    "\n",
    "def append_file(ssh_client, local_path, remote_path, temp_path):\n",
    "    # Step 1: Transfer the file to a temporary location\n",
    "    upload_files(ssh_client, local_path, temp_path)\n",
    "\n",
    "    temp_path = temp_path.replace(\"/\", \"\\\\\")\n",
    "    remote_path = remote_path.replace(\"/\", \"\\\\\")\n",
    "\n",
    "    # Step 2: Append the content of the temporary file to the target file\n",
    "    command = f'type {temp_path} >> {remote_path} & del {temp_path}'\n",
    "    stdin, stdout, stderr = ssh_client.exec_command(command)\n",
    "    exit_status = stdout.channel.recv_exit_status()  # Wait for the command to complete\n",
    "    \n",
    "    # Reading the output of the command\n",
    "    output = stdout.read().decode('utf-8')\n",
    "    error = stderr.read().decode('utf-8')\n",
    "\n",
    "    # Check if command was successful\n",
    "    if exit_status == 0:\n",
    "        print(\"Command executed successfully\")\n",
    "    else:\n",
    "        print(f\"Command failed with exit status {exit_status}\")\n",
    "\n",
    "    # Optional: Print the outputs for debugging or logging\n",
    "    if output:\n",
    "        print(\"Output:\", output)\n",
    "        \n",
    "    if error:\n",
    "        print(\"Error:\", error)\n",
    "\n",
    "    return not exit_status\n",
    "\n",
    "def download_files(ssh_client, remote_path, local_path):\n",
    "    with SCPClient(ssh_client.get_transport()) as scp:\n",
    "        scp.get(remote_path, local_path)\n",
    "\n",
    "# Optional: Execute a command or run a script on the remote machine\n",
    "# stdin, stdout, stderr = ssh_client.exec_command('python /path/to/remote/script.py')\n",
    "# print(stdout.read().decode())  # Assuming the script has output\n",
    "\n",
    "def close_client(ssh_client):\n",
    "    # Close the SSH connection\n",
    "    ssh_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from dateutil import parser\n",
    "\n",
    "server = 'sshhop.hopto.org'\n",
    "port = 22\n",
    "user = 'mum'\n",
    "password = '1234'\n",
    "\n",
    "ticker = \"TSLA\"\n",
    "\n",
    "def download_tweets(file_path):\n",
    "    ssh_client = create_ssh_client(server, port, user, password)\n",
    "    download_files(ssh_client, f'/C:/Users/Mum/Documents/news_aggregation_ipynb/{file_path}', file_path)\n",
    "    close_client(ssh_client)\n",
    "\n",
    "def upload_processed_tweets(localprocessed_file_name, final_tweets_file_name):\n",
    "    ssh_client = create_ssh_client(server, port, user, password)\n",
    "    final_tweets_file_path = f\"C:/Users/Mum/Documents/news_aggregation_ipynb/{final_tweets_file_name}\"\n",
    "    temp_final_tweets_file_path = f\"C:/Users/Mum/Documents/news_aggregation_ipynb/temp_{final_tweets_file_name}\"\n",
    "    print(final_tweets_file_path)\n",
    "    print(temp_final_tweets_file_path)\n",
    "    result = append_file(ssh_client, f'./{localprocessed_file_name}', final_tweets_file_path, temp_final_tweets_file_path)\n",
    "    # Delete the file\n",
    "    close_client(ssh_client)\n",
    "    return result\n",
    "\n",
    "def compare_tweets_and_return_new(tweets_file_path, final_tweets_file_path):\n",
    "    # Download recent tweets\n",
    "    download_tweets(tweets_file_path)\n",
    "    download_tweets(final_tweets_file_path)\n",
    "\n",
    "    # Load IDs from the final tweets file\n",
    "    final_tweet_ids = set()\n",
    "    with open(final_tweets_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            final_tweet_ids.add(tweet['id'])\n",
    "\n",
    "    # Load tweets from the initial file and filter out those that exist in the final file\n",
    "    new_tweets = []\n",
    "    with open(tweets_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            if tweet['id'] not in final_tweet_ids:\n",
    "                # Aggregate created_at and date, then convert to datetime\n",
    "                post_time = tweet.get('created_at') or tweet.get('date')\n",
    "                tweet['post_time'] = parser.parse(post_time)\n",
    "                new_tweets.append(tweet)\n",
    "\n",
    "    new_tweets.sort(key=lambda x: x['post_time'], reverse=True)\n",
    "    return new_tweets\n",
    "\n",
    "def save_processed_tweets(localprocessed_file_path, tweets):\n",
    "    with open(localprocessed_file_path, 'w', encoding='utf-8') as file:\n",
    "        for tweet in tweets:\n",
    "            # Create a new dictionary with only the required fields\n",
    "            filtered_tweet = {\n",
    "                'id': tweet['id'],  # Assuming each tweet has a unique 'id'\n",
    "                'date': tweet.get('date', tweet.get('created_at', '')),\n",
    "                'cleanContent': tweet.get('cleanContent', ''),\n",
    "                'rawContent': tweet.get('rawContent', ''),\n",
    "                'url': tweet.get('url', ''),\n",
    "                'emotion': tweet.get('emotion', ''),\n",
    "                'sentiment': tweet.get('sentiment', '')\n",
    "            }\n",
    "            json.dump(filtered_tweet, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "# Calculate the cutoff time for the last 30 minutes\n",
    "# cutoff_time = datetime.utcnow().replace(tzinfo=pytz.utc) - timedelta(minutes=30)\n",
    "\n",
    "twitter_file_path = f'{ticker}_tweets.jsonl'\n",
    "final_twitter_file_path = f'final_{twitter_file_path}'\n",
    "stocktweets_file_path = f'{ticker}_stocktweets.jsonl'\n",
    "final_stocktweets_file_path = f'final_{stocktweets_file_path}'\n",
    "\n",
    "def process_and_save_tweets(tweets, ticker, final_tweets_file_path, localprocessed_file_path):\n",
    "    sentiment_results = []\n",
    "    emotion_results = []\n",
    "    counter = 0\n",
    "    to_save = []\n",
    "\n",
    "    for i, tweet in enumerate(tqdm(tweets, desc=\"Analyzing Tweets\")):\n",
    "        clean_content = tweet.get('cleanContent', '')\n",
    "        \n",
    "        sentiment = classify_sentiment(clean_content)\n",
    "        emotion = classify_emotion(clean_content)\n",
    "        \n",
    "        sentiment_results.append(f\"{i}: \" + sentiment)\n",
    "        emotion_results.append(f\"{i}: \" + emotion)\n",
    "        \n",
    "        tweet['sentiment'] = sentiment\n",
    "        tweet['emotion'] = emotion\n",
    "        to_save.append(tweet)\n",
    "    \n",
    "        counter += 1\n",
    "        # Save every 10 tweets or on the last tweet\n",
    "        if counter % 20 == 0 or i == len(tweets) - 1:\n",
    "            save_processed_tweets(localprocessed_file_path, to_save)\n",
    "            result = upload_processed_tweets(localprocessed_file_path, final_tweets_file_path)    \n",
    "            if result:\n",
    "                print(f\"Saved up to tweet {i+1}\")\n",
    "                to_save = []  # Reset the list for the next batch\n",
    "\n",
    "    # Print results\n",
    "    print(sentiment_results)\n",
    "    print(emotion_results)\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        print(f\"{i}: \" + tweet.get('cleanContent', ''))\n",
    "    \n",
    "    for i, tweet in enumerate(stocktweets):\n",
    "        print(f\"{i}: \" + tweet.get('cleanContent', ''))\n",
    "\n",
    "counter = 0\n",
    "while True:\n",
    "    print(\"downloading\")\n",
    "    tweets = compare_tweets_and_return_new(twitter_file_path, final_twitter_file_path)\n",
    "    print(\"downloaded\")\n",
    "    print(\"downloading\")\n",
    "    stocktweets = compare_tweets_and_return_new(stocktweets_file_path, final_stocktweets_file_path)\n",
    "    print(\"downloaded\")\n",
    "\n",
    "    print(\"Twitter:\", ticker)\n",
    "    process_and_save_tweets(tweets, ticker, final_twitter_file_path, f'processed_{twitter_file_path}')\n",
    "    print(\"Stocktwits:\", ticker)\n",
    "    process_and_save_tweets(stocktweets, ticker, final_stocktweets_file_path, f'processed_{twitter_file_path}')\n",
    "\n",
    "    counter += 1\n",
    "    print(f\"+-+-+-+-+-+-+-+-+-+-Cycles Completed: {counter}+-+-+-+-+-+-+-+-+-+-\")\n",
    "    for i in range(60):\n",
    "        time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = NewsPlease.from_url('https://finance.yahoo.com/news/alaska-airlines-begun-flying-boeing-150009733.html')\n",
    "\n",
    "# Split the article text into sentences\n",
    "sentences = sent_tokenize(article.maintext)\n",
    "print(\"Sentences in the article: \", len(sentences))\n",
    "\n",
    "# Classify emotion for each sentence with a progress bar\n",
    "emotion_results = []\n",
    "for i, sentence in enumerate(tqdm(sentences, desc=\"Processing Sentences\")):\n",
    "    emotion = classify_sentiment(sentence)\n",
    "    emotion_results.append(emotion)\n",
    "\n",
    "# Do something with the results\n",
    "print(emotion_results)\n",
    "\n",
    "# print(article.maintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
