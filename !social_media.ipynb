{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/vladkens/twscrape.git\n",
    "# %pip install langdetect twscrape parsel tqdm pandas\n",
    "# !git clone https://github.com/AI4Finance-Foundation/FinNLP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to clean the contents of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mum\\AppData\\Local\\Temp\\ipykernel_8204\\799825978.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def clean_tweet_content(text):\n",
    "    \"\"\"Remove mentions, links, emojis, newlines, and specific search terms from tweet text.\"\"\"\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'https?://\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with space\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'\\$\\w+', '', text)  # Remove cashtags\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\"\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                                u\"\\U0001F700-\\U0001F77F\"\n",
    "                                u\"\\U0001F780-\\U0001F7FF\"\n",
    "                                u\"\\U0001F800-\\U0001F8FF\"\n",
    "                                u\"\\U0001F900-\\U0001F9FF\"\n",
    "                                u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                                u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                                u\"\\U00002702-\\U000027B0\"\n",
    "                                u\"\\U000024C2-\\U0001F251\"\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)  # Apply regex to remove emojis\n",
    "    \n",
    "    # Normalize spaces - replace two or more spaces with a single space\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def is_spam(tweet, df):\n",
    "    text = tweet['rawContent']\n",
    "    # Define a list of promotional phrases to search for\n",
    "    promotional_phrases = [\n",
    "        \"join us\",\n",
    "        \"find out more\",\n",
    "        \"Sign up now\",\n",
    "        \"become a member\",\n",
    "        \"Subscribe for updates\",\n",
    "        \"Limited time offer\",\n",
    "        \"Exclusive access\",\n",
    "        \"Free trial\",\n",
    "        \"Don't miss out\",\n",
    "        \"Last chance\",\n",
    "        \"Early bird discount\",\n",
    "        \"Save big\",\n",
    "        \"Register today\",\n",
    "        \"Unlock benefits\",\n",
    "        \"Special promotion\",\n",
    "        \"Available for a limited time\",\n",
    "        \"Claim your offer\",\n",
    "        \"Act now\",\n",
    "        \"Exclusive deals\",\n",
    "        \"Get started\",\n",
    "        \"Offer ends soon\",\n",
    "        \"While supplies last\",\n",
    "        \"Money-back guarantee\",\n",
    "        \"Check out now\",\n",
    "        \"Check out now...\",\n",
    "        \"Check out now..\",\n",
    "        \"Check out now.\",\n",
    "        \"Check out\",\n",
    "        \"Pay a visit\",\n",
    "        \"Most profitable trading community.\",\n",
    "        \"Join the\",\n",
    "        \"Best trading community\",\n",
    "        \"Free access\",\n",
    "        \"trading community\",\n",
    "        \"our ChatRoom\",\n",
    "        \"join here\",\n",
    "        \"Join our\",\n",
    "        \"Check us out\",\n",
    "        \"Daily ALERTS\",\n",
    "        \"trading alerts\",\n",
    "        \"Free chatroom\",\n",
    "        \"Learn To Trade\",\n",
    "        \"limited time\",\n",
    "        \"CHATROOM\",\n",
    "        \"We provide\",\n",
    "        \"with us\",\n",
    "        \"OUR FREE\",\n",
    "        \"Learn how\",\n",
    "        'chat room',\n",
    "        \"Top analyst\",\n",
    "        \"price target\"\n",
    "    ]\n",
    "    \n",
    "    promotional_words = [\n",
    "        'trading',\n",
    "        'community',\n",
    "        'alerts'\n",
    "    ]\n",
    "    \n",
    "    # Create a combined regular expression from the list of phrases\n",
    "    regex_pattern = '|'.join(map(re.escape, promotional_phrases))\n",
    "    # Search for any of the phrases in the text\n",
    "    contains_promotion = re.search(regex_pattern, text, re.IGNORECASE) is not None\n",
    "    \n",
    "        # Create a combined regular expression from the list of phrases with optional spaces between words\n",
    "    regex_pattern = '|'.join(['\\\\s*'.join(map(re.escape, phrase.split())) for phrase in promotional_words])\n",
    "    # Find all occurrences of the phrases in the text and check if there are two or more\n",
    "    contains_two_or_more_promotion_words = len(re.findall(regex_pattern, text, re.IGNORECASE)) >= 2\n",
    "    \n",
    "    # Regular expression to find cashtags (e.g., $AAPL)\n",
    "    cashtag_pattern = r'\\$[A-Za-z]+'\n",
    "    # Find all occurrences of cashtags in the text\n",
    "    cashtags = re.findall(cashtag_pattern, text)\n",
    "    # Check if there are more than four cashtags\n",
    "    contains_more_than_four_cashtags = len(cashtags) > 4\n",
    "\n",
    "    # _________________________________________________________________________________\n",
    "    \n",
    "    # A simple heuristic: check if the text has at least three alphabetic characters\n",
    "    is_too_short = len(re.findall(\"[a-zA-Z]\", text)) < 5\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Assuming `tweet` is a dict representing an incoming tweet for the spam check function\n",
    "    user_id = tweet['user']['id']\n",
    "    date_column = df.index.name\n",
    "    timestamp = pd.to_datetime(tweet[date_column])\n",
    "\n",
    "    # print(timestamp.tz)\n",
    "    \n",
    "    # Define the 24-hour lookback period\n",
    "    start_time = timestamp - pd.Timedelta(days=1)\n",
    "    # print(df.index)\n",
    "    # print(start_time.tz)\n",
    "\n",
    "    # # print(type(timestamp))\n",
    "    # # print(type(start_time))\n",
    "    # print(pd.to_datetime(df.index))\n",
    "\n",
    "    # Filter for tweets by the same user in the past 24 hours\n",
    "    recent_tweets = df[(df['user_id'] == user_id) & \n",
    "                   (pd.to_datetime(df.index, utc=True) > start_time) & \n",
    "                   (pd.to_datetime(df.index, utc=True) <= timestamp)]\n",
    "    \n",
    "    # If text is x percent similar then treat as duplicate.\n",
    "    similar_texts = any(SequenceMatcher(None, text, rt).ratio() > 0.98 for rt in recent_tweets['rawContent'])\n",
    "    \n",
    "    # Check conditions\n",
    "    if len(recent_tweets) > 15:\n",
    "        tweet['reason'] = 'Spam detected due to excessive tweeting.'\n",
    "        return True\n",
    "    elif similar_texts:\n",
    "        tweet['reason'] = 'Spam detected due to duplicate text.'\n",
    "        return True\n",
    "    \n",
    "    return  contains_promotion or contains_two_or_more_promotion_words or contains_more_than_four_cashtags or is_too_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Tweets from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_search_term = \"Tesla OR TSLA\"\n",
    "ticker=\"TSLA\"\n",
    "tweets_limit=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twscrape import API, gather\n",
    "from twscrape.logger import set_log_level\n",
    "import json\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "import re\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "\n",
    "# ####################################################################################\n",
    "# Gather the tweets\n",
    "# ####################################################################################\n",
    "async def main():\n",
    "    api = API()  # or API(\"path-to.db\") - default is `accounts.db`\n",
    "\n",
    "    # ADD ACCOUNTS (for CLI usage see BELOW)\n",
    "    await api.pool.add_account(\"DarOho4050\", \"asdfghjkl1!\", \"we444465@gmail.com\", \"mail_pass1\")\n",
    "    await api.pool.login_all()\n",
    "\n",
    "# search (latest tab)\n",
    "    tweets = await gather(api.search(st_search_term, limit=tweets_limit))  # list[Tweet]\n",
    "    # print(tweets)\n",
    "    \n",
    "    def user_to_dict(user):\n",
    "        return {\n",
    "            'id': user.id,\n",
    "            'id_str': user.id_str,\n",
    "            'url': user.url,\n",
    "            'username': user.username,\n",
    "            'displayname': user.displayname,\n",
    "            'rawDescription': user.rawDescription,\n",
    "            'created': user.created.isoformat() if user.created else None,            \n",
    "        }\n",
    "    \n",
    "    def tweet_to_dict(tweet):\n",
    "        return {\n",
    "            'id': tweet.id,\n",
    "            'date': tweet.date.isoformat(),\n",
    "            'rawContent': tweet.rawContent,\n",
    "            'cleanContent': clean_tweet_content(tweet.rawContent),\n",
    "            'url': tweet.url,\n",
    "            'user': user_to_dict(tweet.user),\n",
    "            'lang': tweet.lang,\n",
    "            'replyCount': tweet.replyCount,\n",
    "            'retweetCount': tweet.retweetCount,\n",
    "            'likeCount': tweet.likeCount,\n",
    "            'quoteCount': tweet.quoteCount\n",
    "        }\n",
    "\n",
    "    # Convert each Tweet instance to a dictionary\n",
    "    tweets_as_dicts = [tweet_to_dict(tweet) for tweet in tweets]\n",
    "    # print(tweets_as_dicts)\n",
    "    return tweets_as_dicts\n",
    "    \n",
    "def write_tweets_to_file(tweets, file_path):\n",
    "    \"\"\"Generic function to write tweets to a specified file with deduplication.\"\"\"\n",
    "    new_tweets_count = 0\n",
    "    existing_ids = set()\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            existing_ids.update(json.loads(line)['id'] for line in file)\n",
    "    except FileNotFoundError:\n",
    "        pass  # It's okay if the file does not exist yet\n",
    "\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        for tweet in tweets:\n",
    "            if tweet['id'] not in existing_ids:\n",
    "                file.write(json.dumps(tweet, ensure_ascii=False) + '\\n')\n",
    "                new_tweets_count += 1\n",
    "                existing_ids.add(tweet['id'])\n",
    "\n",
    "    return new_tweets_count\n",
    "\n",
    "def does_tweet_match_criteria(tweet_content, search_terms):\n",
    "    \"\"\"Check if tweet content matches the search criteria using regex for flexibility.\n",
    "       Supports 'AND' and 'OR' logic in search terms.\"\"\"\n",
    "    # Split the search terms based on 'AND' and 'OR' logic\n",
    "    or_groups = [group.strip() for group in search_terms.split('OR')]\n",
    "    \n",
    "    for group in or_groups:\n",
    "        # For each 'OR' group, check if any of the 'AND' terms are all present\n",
    "        and_terms = group.split('AND')\n",
    "        if all(re.search(r'\\b' + term.strip() + r'\\b', tweet_content, re.IGNORECASE) for term in and_terms):\n",
    "            return True  # If all 'AND' terms in any 'OR' group match, return True\n",
    "    return False  # If none of the groups match, return False\n",
    "\n",
    "def filter_tweets(tweets, search_term, language=\"en\", df=[]):\n",
    "    \"\"\"Filter out Tweets that do not contain all of the provided search terms.\"\"\"\n",
    "    english_tweets = []\n",
    "    filtered_tweets = []  # Tweets filtered out\n",
    "    for tweet in tweets:\n",
    "        if does_tweet_match_criteria(tweet['rawContent'], search_term):\n",
    "            try:\n",
    "                if detect(tweet['cleanContent']) == 'en' and not is_spam(tweet, df):\n",
    "                    english_tweets.append(tweet)\n",
    "                else:\n",
    "                    filtered_tweets.append(tweet)\n",
    "            except LangDetectException:\n",
    "                # Assume English if detection fails\n",
    "                filtered_tweets.append(tweet)\n",
    "        else:\n",
    "            filtered_tweets.append(tweet)\n",
    "    return english_tweets, filtered_tweets\n",
    "\n",
    "def append_df(array, df, unique_col='id'):\n",
    "    array_df = pd.DataFrame(array)\n",
    "    updated_df = pd.concat([df.reset_index(), array_df]).drop_duplicates(subset=[unique_col], keep='first')\n",
    "    \n",
    "    new_tweets_count = len(updated_df) - len(df)\n",
    "    return new_tweets_count, updated_df\n",
    "\n",
    "async def main_tweets(df):\n",
    "    tweets = await main()  # Assume this fetches a list of tweet dictionaries\n",
    "    english_tweets, filtered_tweets = filter_tweets(tweets, st_search_term, df=df)\n",
    "\n",
    "    new_english_count, df = append_df(english_tweets, df, unique_col='id')\n",
    "    df.to_json(f'{ticker}_tweets.jsonl', orient='records', lines=True, date_format='iso')\n",
    "    \n",
    "    # new_english_count = write_tweets_to_file(english_tweets, f'{ticker}_tweets.jsonl')\n",
    "    filtered_count = write_tweets_to_file(filtered_tweets, f'trash_{ticker}_tweets.jsonl')\n",
    "    \n",
    "    total_processed = len(english_tweets) + len(filtered_tweets)\n",
    "    \n",
    "    if total_processed > 0:\n",
    "        english_percentage = (new_english_count / total_processed) * 100\n",
    "        filtered_percentage = (filtered_count / total_processed) * 100\n",
    "        print(f\"Processed {total_processed} tweets. New Tweets/New Percentage: {new_english_count + filtered_count}:{(english_percentage + filtered_percentage):.2f}% New English: {english_percentage:.2f}%. Filtered: {filtered_percentage:.2f}%.\")\n",
    "    else: \n",
    "        print(\"No tweets to process.\")\n",
    "    \n",
    "    return {\"new_tweets\": new_english_count + filtered_count, \"total_tweets\": total_processed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Tweets from StockTwits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Stocktwits\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{os.getcwd()}\\FinNLP')\n",
    "from finnlp.data_sources.social_media.stocktwits_streaming import Stocktwits_Streaming\n",
    "\n",
    "st_search_term = f\"{ticker}\"\n",
    "file_path = f'./{ticker}_stocktwits.jsonl'\n",
    "\n",
    "\n",
    "pages = 1\n",
    "config = {\n",
    "    \"max_retry\": 5\n",
    "}\n",
    "\n",
    "async def main_stocktwits(df):\n",
    "    downloader = Stocktwits_Streaming(config)\n",
    "    downloader.download_streaming_stock(ticker, pages)\n",
    "\n",
    "    stocktweets = downloader.dataframe\n",
    "    stocktweets = stocktweets.rename(columns={'body': 'rawContent'})\n",
    "    stocktweets = stocktweets.to_dict('records')\n",
    "    # print(stocktweets)\n",
    "\n",
    "    stocktweets = [{**tweet, 'cleanContent': clean_tweet_content(tweet['rawContent'])} for tweet in stocktweets]\n",
    "\n",
    "    english_tweets, filtered_tweets = filter_tweets(stocktweets, st_search_term, df=df)\n",
    "    \n",
    "\n",
    "    new_english_count, df = append_df(english_tweets, df, unique_col='id')\n",
    "    df.to_json(f'{ticker}_stocktweets.jsonl', orient='records', lines=True, date_format='iso')\n",
    "    \n",
    "    # new_english_count = write_tweets_to_file(english_tweets, f'{ticker}_stocktweets.jsonl')\n",
    "    filtered_count = write_tweets_to_file(filtered_tweets, f'trash_{ticker}_stocktweets.jsonl')\n",
    "\n",
    "    total_processed = len(english_tweets) + len(filtered_tweets)\n",
    "\n",
    "    if total_processed > 0:\n",
    "        english_percentage = (new_english_count / total_processed) * 100\n",
    "        filtered_percentage = (filtered_count / total_processed) * 100\n",
    "        print(f\"Processed {total_processed} stocktwits. New Tweets/New Percentage: {new_english_count + filtered_count}:{(english_percentage + filtered_percentage):.2f}% New English: {english_percentage:.2f}%. Filtered: {filtered_percentage:.2f}%.\")\n",
    "    else: \n",
    "        print(\"No tweets to process.\")\n",
    "        \n",
    "    return {\"new_tweets\": new_english_count + filtered_count, \"total_tweets\": total_processed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run both at the same time (indefinity every 60 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-02 21:31:04.535\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36madd_account\u001b[0m:\u001b[36m76\u001b[0m - \u001b[33m\u001b[1mAccount DarOho4050 already exists\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 tweets. New Tweets/New Percentage: 15:75.00% New English: 20.00%. Filtered: 55.00%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30 stocktwits. New Tweets/New Percentage: 22:73.33% New English: 0.00%. Filtered: 73.33%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-02 21:33:08.264\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36madd_account\u001b[0m:\u001b[36m76\u001b[0m - \u001b[33m\u001b[1mAccount DarOho4050 already exists\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 17 tweets. New Tweets/New Percentage: 8:47.06% New English: 11.76%. Filtered: 35.29%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30 stocktwits. New Tweets/New Percentage: 0:0.00% New English: 0.00%. Filtered: 0.00%.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas  as pd\n",
    "\n",
    "async def save_to_jsonl(data, file_path='tweets_stats.jsonl'):\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        file.write(json.dumps(data) + '\\n')\n",
    "\n",
    "# Load tweets and convert the date column to datetime\n",
    "tweets_df = pd.read_json(f'{ticker}_tweets.jsonl', lines=True)\n",
    "date_column = 'created_at' if 'created_at' in tweets_df.columns else 'date'\n",
    "tweets_df.set_index(date_column, inplace=True)\n",
    "tweets_df['user_id'] = tweets_df['user'].apply(lambda x: x['id'] if 'id' in x else None)\n",
    "\n",
    "# Repeat for stocktweets, if processing separately\n",
    "stocktweets_df = pd.read_json(f'{ticker}_stocktweets.jsonl', lines=True)\n",
    "date_column = 'created_at' if 'created_at' in stocktweets_df.columns else 'date'\n",
    "stocktweets_df.set_index(date_column, inplace=True)\n",
    "stocktweets_df['user_id'] = stocktweets_df['user'].apply(lambda x: x['id'] if 'id' in x else None)\n",
    "\n",
    "while True:\n",
    "    tweet_data = await main_tweets(tweets_df)\n",
    "    stocktwit_data = await main_stocktwits(stocktweets_df)\n",
    "    \n",
    "    combined_data = {\n",
    "        \"twitter\": tweet_data,\n",
    "        \"stocktwits\": stocktwit_data,\n",
    "        \"timestamp\": datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    }\n",
    "    \n",
    "    await save_to_jsonl(combined_data)\n",
    "    \n",
    "    time.sleep(60)\n",
    "    time.sleep(10)\n",
    "    time.sleep(10)  \n",
    "    time.sleep(10)\n",
    "    time.sleep(10)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
