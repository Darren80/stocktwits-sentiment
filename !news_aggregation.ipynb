{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqQCqE-c-sGc"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PskG05TlIa1W",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install yfinance --upgrade --no-cache-dir\n",
    "!pip install finnhub-python\n",
    "!pip3 install news-please\n",
    "\n",
    "!pip install torch\n",
    "!pip install peft\n",
    "!pip install -U accelerate\n",
    "!pip install transformers bitsandbytes\n",
    "!pip install pynvml\n",
    "!pip install gnews\n",
    "!pip install selenium\n",
    "!pip install webdriver_manager\n",
    "!pip install --upgrade undetected-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\bow33/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bow33/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News from other sources (Google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bow33\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install newspaper3k\n",
    "from gnews import GNews\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "search_term = \"Tesla stock\"\n",
    "google_news = GNews()\n",
    "def get_news(search_terms):\n",
    "    stock_news = google_news.get_news(search_terms)\n",
    "    # Define a function to convert date strings into datetime objects\n",
    "    def parse_date(date_str):\n",
    "        # Example date: \"Thu, 08 Feb 2024 15:35:00 GMT\"\n",
    "        return datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S GMT')\n",
    "\n",
    "    # Sort stock_news by parsed 'publish date'\n",
    "    stock_news = sorted(stock_news, key=lambda x: parse_date(x['published date']), reverse=True)\n",
    "    \n",
    "    print(stock_news)\n",
    "    return stock_news\n",
    "\n",
    "def get_content(url, homepage_url):\n",
    "    article = google_news.get_full_article(url, homepage_url)\n",
    "    return article\n",
    "\n",
    "import json\n",
    "\n",
    "def write_articles_to_file(articles, file_path):\n",
    "    \"\"\"Function to update or add articles while preserving the 'content' key for existing articles, without safeguarding the 'url'.\"\"\"\n",
    "    try:\n",
    "        # Load existing articles from file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            existing_articles = {json.loads(line)['title'] + json.loads(line)['published date']: json.loads(line)\n",
    "                                 for line in file}\n",
    "    except FileNotFoundError:\n",
    "        existing_articles = {}\n",
    "\n",
    "    updated = False  # Flag to track if any article was updated or added\n",
    "\n",
    "    for article in articles:\n",
    "        key = article['title'] + article['published date']\n",
    "        # Check if the article exists and preserve 'content' if it does\n",
    "        if key in existing_articles and 'content' in existing_articles[key]:\n",
    "            article['content'] = existing_articles[key]['content']\n",
    "        # This updates or adds the new article, including updating 'url'\n",
    "        existing_articles[key] = article\n",
    "        updated = True\n",
    "\n",
    "    # Write back to the file only if there was an update\n",
    "    if updated:\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            for article in existing_articles.values():\n",
    "                file.write(json.dumps(article, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    return len(articles)  # Returning count of processed articles for simplicity\n",
    "\n",
    "def load_articles_with_empty_content(file_path):\n",
    "    \"\"\"Load articles from a file, returning only those with empty or missing 'content'.\"\"\"\n",
    "    articles_with_empty_content = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                article = json.loads(line)\n",
    "                # Check if 'content' is missing or empty\n",
    "                if not article.get('content'):\n",
    "                    articles_with_empty_content.append(article)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No file found at {file_path}\")\n",
    "    \n",
    "    return articles_with_empty_content\n",
    "\n",
    "def load_articles_with_full_content(file_path):\n",
    "    \"\"\"Load articles from a file, returning only those with empty or missing 'content'.\"\"\"\n",
    "    articles_with_empty_content = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                article = json.loads(line)\n",
    "                # Check if 'content' is missing or empty\n",
    "                if article.get('content'):\n",
    "                    articles_with_empty_content.append(article)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No file found at {file_path}\")\n",
    "    \n",
    "    return articles_with_empty_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once news is collected start the scraping process to get their contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ['barrons.com', 'wsj.com']\n",
    "\n",
    "# Only do x articles before continuing (for loop, a skipped article does not count as a step)\n",
    "def main_news(stock_news):\n",
    "    print(len(stock_news))\n",
    "    articles_to_scrape = 10\n",
    "    for i, article in enumerate(stock_news):\n",
    "        \n",
    "        articles_to_scrape -= 1\n",
    "        print(article['published date'])\n",
    "        \n",
    "        if any(excluded_domain in article['publisher']['href'] for excluded_domain in exclude):\n",
    "            print(\"Closed due to  domain exclusion: \", article['publisher']['href'])\n",
    "            articles_to_scrape += 1\n",
    "            continue\n",
    "        \n",
    "        article['content'] = get_content(article['url'], article['publisher']['href']).text\n",
    "        article = [article]\n",
    "        \n",
    "        # print(\"+_+_+_+_Content: \", article[0]['content'])\n",
    "        \n",
    "        write_articles_to_file(article, \"tesla_stock.jsonl\")\n",
    "        print(i + 1, \" out of \", len(stock_news))\n",
    "        \n",
    "        if articles_to_scrape == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the process every x seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    stock_news = get_news(search_term)\n",
    "    write_articles_to_file(stock_news, \"tesla_news.jsonl\")\n",
    "\n",
    "    unscrapped_stock_news = load_articles_with_empty_content(\"tesla_news.jsonl\")\n",
    "    main_news(stock_news)\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 'content' of each article using a web scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkGpnvDljCKE",
    "outputId": "984b4f69-854e-4e90-dda9-890ab63de2a7"
   },
   "outputs": [],
   "source": [
    "# import yfinance as yf\n",
    "# import finnhub\n",
    "# finnhub_client = finnhub.Client(api_key=\"cmr6gghr01qvmr5q06u0cmr6gghr01qvmr5q06ug\")\n",
    "\n",
    "# msft = yf.Ticker(\"MSFT\")\n",
    "# msft.news\n",
    "# # Keeping only 'title' and 'link' in each item\n",
    "# msft_news_filtered = [{\"title\": item[\"title\"], \"link\": item[\"link\"]} for item in msft.news]\n",
    "\n",
    "# print(msft_news_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2hMNfEgCn-X"
   },
   "source": [
    "**FinForcaster App.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKZwbTC3W9S6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import finnhub\n",
    "import torch\n",
    "# import gradio as gr\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pynvml import *\n",
    "from peft import PeftModel\n",
    "from collections import defaultdict\n",
    "from datetime import date, datetime, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=\"cmr6gghr01qvmr5q06u0cmr6gghr01qvmr5q06ug\")\n",
    "\n",
    "def print_gpu_utilization():\n",
    "\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def get_curday():\n",
    "\n",
    "    return date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def n_weeks_before(date_string, n):\n",
    "    # date = datetime.strptime(date_string, \"%Y-%m-%d\") - timedelta(days=7*n)\n",
    "    date = datetime.strptime(date_string, \"%Y-%m-%d\") - timedelta(days=n)\n",
    "    return date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def n_days_before(date_string, n):\n",
    "    date = datetime.strptime(date_string, \"%Y-%m-%d\") - timedelta(days=n)\n",
    "    return date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def get_stock_data(stock_symbol, steps):\n",
    "\n",
    "    stock_data = yf.download(stock_symbol, steps[0], steps[-1])\n",
    "    if len(stock_data) == 0:\n",
    "        raise gr.Error(f\"Failed to download stock price data for symbol {stock_symbol} from yfinance!\")\n",
    "\n",
    "#     print(stock_data)\n",
    "\n",
    "    dates, prices = [], []\n",
    "    available_dates = stock_data.index.format()\n",
    "\n",
    "    for date in steps[:-1]:\n",
    "        for i in range(len(stock_data)):\n",
    "            if available_dates[i] >= date:\n",
    "                prices.append(stock_data['Close'][i])\n",
    "                dates.append(datetime.strptime(available_dates[i], \"%Y-%m-%d\"))\n",
    "                break\n",
    "\n",
    "    dates.append(datetime.strptime(available_dates[-1], \"%Y-%m-%d\"))\n",
    "    prices.append(stock_data['Close'][-1])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Start Date\": dates[:-1], \"End Date\": dates[1:],\n",
    "        \"Start Price\": prices[:-1], \"End Price\": prices[1:]\n",
    "    })\n",
    "\n",
    "\n",
    "def get_news(symbol, data, profile):\n",
    "    \n",
    "    news_list = []\n",
    "    filtered_company_name = ' '.join(profile['name'].split()[:1])\n",
    "    print(profile)\n",
    "\n",
    "    for end_date, row in data.iterrows():\n",
    "        start_date = row['Start Date'].strftime('%Y-%m-%d')\n",
    "        end_date = row['End Date'].strftime('%Y-%m-%d')\n",
    "#         print(symbol, ': ', start_date, ' - ', end_date)\n",
    "        # time.sleep(1) # control qpm\n",
    "        weekly_news = finnhub_client.company_news(symbol, _from=start_date, to=end_date)\n",
    "        if len(weekly_news) == 0:\n",
    "            raise gr.Error(f\"No company news found for symbol {symbol} from finnhub!\")\n",
    "        weekly_news = [\n",
    "            {\n",
    "                \"date\": datetime.fromtimestamp(n['datetime']).strftime('%Y-%m-%d %H:%M'),\n",
    "                \"headline\": n['headline'],\n",
    "                \"summary\": n['summary'],\n",
    "                \"url\": n['url'],\n",
    "                \"source\": n['source']\n",
    "            } \n",
    "            for n in weekly_news\n",
    "            if \"zacks.com\" not in n['summary'].lower() and \n",
    "               (symbol.lower() in n['headline'].lower() or filtered_company_name.lower() in n['headline'].lower())\n",
    "        ]\n",
    "        weekly_news.sort(key=lambda x: x['date'])\n",
    "        news_list.append(json.dumps(weekly_news))\n",
    "\n",
    "    data['News'] = news_list\n",
    "\n",
    "    # additions:\n",
    "    total_news_count = sum(len(json.loads(weekly_news)) for weekly_news in news_list)\n",
    "    print(\"Number of news items after filtering:\", total_news_count) \n",
    "\n",
    "    # To save the news to a file, use:\n",
    "    print_or_save_news(data['News'], f\"{symbol}_news.txt\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def print_or_save_news(news_list, filename=None):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"\")\n",
    "    for news_items_str in news_list:\n",
    "        news_items = json.loads(news_items_str)  # Load the JSON string into a list of dictionaries\n",
    "        for news in news_items:\n",
    "            news_str = f\"Date: {news['date']}\\nHeadline: {news['headline']}\\nSummary: {news['summary']}\\nURL: {news['url']}\\nSource: {news['source']}\\n\\n\"\n",
    "            if filename:\n",
    "                with open(filename, 'a', encoding='utf-8') as file:\n",
    "                    file.write(news_str)\n",
    "            else:\n",
    "                print(news_str)\n",
    "\n",
    "def get_company_prompt(symbol, profile):\n",
    "\n",
    "    # profile = finnhub_client.company_profile2(symbol=symbol)\n",
    "    if not profile:\n",
    "        raise gr.Error(f\"Failed to find company profile for symbol {symbol} from finnhub!\")\n",
    "\n",
    "    company_template = \"[Company Introduction]:\\n\\n{name} is a leading entity in the {finnhubIndustry} sector. Incorporated and publicly traded since {ipo}, the company has established its reputation as one of the key players in the market. As of today, {name} has a market capitalization of {marketCapitalization:.2f} in {currency}, with {shareOutstanding:.2f} shares outstanding.\" \\\n",
    "        \"\\n\\n{name} operates primarily in the {country}, trading under the ticker {ticker} on the {exchange}. As a dominant force in the {finnhubIndustry} space, the company continues to innovate and drive progress within the industry.\"\n",
    "\n",
    "    formatted_str = company_template.format(**profile)\n",
    "\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "def get_prompt_by_row(symbol, row):\n",
    "\n",
    "    start_date = row['Start Date'] if isinstance(row['Start Date'], str) else row['Start Date'].strftime('%Y-%m-%d')\n",
    "    end_date = row['End Date'] if isinstance(row['End Date'], str) else row['End Date'].strftime('%Y-%m-%d')\n",
    "    term = 'increased' if row['End Price'] > row['Start Price'] else 'decreased'\n",
    "    head = \"From {} to {}, {}'s stock price {} from {:.2f} to {:.2f}. Company news during this period are listed below:\\n\\n\".format(\n",
    "        start_date, end_date, symbol, term, row['Start Price'], row['End Price'])\n",
    "\n",
    "    news = json.loads(row[\"News\"])\n",
    "    news = [\"[Headline]: {}\\n[Summary]: {}\\n\".format(\n",
    "        n['headline'], n['summary']) for n in news if n['date'][:8] <= end_date.replace('-', '') and \\\n",
    "        not n['summary'].startswith(\"Looking for stock market analysis and research with proves results?\")]\n",
    "\n",
    "    basics = json.loads(row['Basics'])\n",
    "    if basics:\n",
    "        basics = \"Some recent basic financials of {}, reported at {}, are presented below:\\n\\n[Basic Financials]:\\n\\n\".format(\n",
    "            symbol, basics['period']) + \"\\n\".join(f\"{k}: {v}\" for k, v in basics.items() if k != 'period')\n",
    "    else:\n",
    "        basics = \"[Basic Financials]:\\n\\nNo basic financial reported.\"\n",
    "\n",
    "    return head, news, basics\n",
    "\n",
    "\n",
    "def sample_news(news, k=5):\n",
    "\n",
    "    return [news[i] for i in sorted(random.sample(range(len(news)), k))]\n",
    "\n",
    "\n",
    "def get_current_basics(symbol, curday):\n",
    "\n",
    "    basic_financials = finnhub_client.company_basic_financials(symbol, 'all')\n",
    "    if not basic_financials['series']:\n",
    "        raise gr.Error(f\"Failed to find basic financials for symbol {symbol} from finnhub!\")\n",
    "\n",
    "    final_basics, basic_list, basic_dict = [], [], defaultdict(dict)\n",
    "\n",
    "    for metric, value_list in basic_financials['series']['quarterly'].items():\n",
    "        for value in value_list:\n",
    "            basic_dict[value['period']].update({metric: value['v']})\n",
    "\n",
    "    for k, v in basic_dict.items():\n",
    "        v.update({'period': k})\n",
    "        basic_list.append(v)\n",
    "\n",
    "    basic_list.sort(key=lambda x: x['period'])\n",
    "\n",
    "    for basic in basic_list[::-1]:\n",
    "        if basic['period'] <= curday:\n",
    "            break\n",
    "\n",
    "    return basic\n",
    "\n",
    "\n",
    "def get_all_prompts_online(symbol, data, curday, profile, with_basics=True):\n",
    "\n",
    "    company_prompt = get_company_prompt(symbol, profile)\n",
    "\n",
    "    prev_rows = []\n",
    "\n",
    "    for row_idx, row in data.iterrows():\n",
    "        head, news, _ = get_prompt_by_row(symbol, row)\n",
    "        prev_rows.append((head, news, None))\n",
    "\n",
    "    prompt = \"\"\n",
    "    for i in range(-len(prev_rows), 0):\n",
    "        prompt += \"\\n\" + prev_rows[i][0]\n",
    "        sampled_news = sample_news(\n",
    "            prev_rows[i][1],\n",
    "            min(5, len(prev_rows[i][1]))\n",
    "        )\n",
    "        if sampled_news:\n",
    "            prompt += \"\\n\".join(sampled_news)\n",
    "        else:\n",
    "            prompt += \"No relative news reported.\"\n",
    "\n",
    "    period = \"{} to {}\".format(curday, n_weeks_before(curday, -1))\n",
    "\n",
    "    if with_basics:\n",
    "        basics = get_current_basics(symbol, curday)\n",
    "        basics = \"Some recent basic financials of {}, reported at {}, are presented below:\\n\\n[Basic Financials]:\\n\\n\".format(\n",
    "            symbol, basics['period']) + \"\\n\".join(f\"{k}: {v}\" for k, v in basics.items() if k != 'period')\n",
    "    else:\n",
    "        basics = \"[Basic Financials]:\\n\\nNo basic financial reported.\"\n",
    "\n",
    "    info = company_prompt + '\\n' + prompt + '\\n' + basics\n",
    "    prompt = info + f\"\\n\\nBased on all the information before {curday}, let's first analyze the positive developments and potential concerns for {symbol}. Come up with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. \" \\\n",
    "        f\"Then make your prediction of the {symbol} stock price movement for next week ({period}). Provide a summary analysis to support your prediction.\"\n",
    "\n",
    "    return info, prompt\n",
    "\n",
    "\n",
    "def construct_prompt(ticker, curday, n_weeks, use_basics):\n",
    "\n",
    "    profile = finnhub_client.company_profile2(symbol=ticker)\n",
    "\n",
    "    try:\n",
    "        steps = [n_weeks_before(curday, n) for n in range(n_weeks + 1)][::-1]\n",
    "    except Exception:\n",
    "        raise gr.Error(f\"Invalid date {curday}!\")\n",
    "\n",
    "    data = get_stock_data(ticker, steps)\n",
    "    data = get_news(ticker, data, profile)\n",
    "    data['Basics'] = [json.dumps({})] * len(data)\n",
    "    # print(data)\n",
    "\n",
    "    info, prompt = get_all_prompts_online(ticker, data, curday, profile, use_basics)\n",
    "\n",
    "    prompt = B_INST + B_SYS + SYSTEM_PROMPT + E_SYS + prompt + E_INST\n",
    "    # print(prompt)\n",
    "\n",
    "    return info, prompt\n",
    "\n",
    "\n",
    "def predict(ticker, date, n_weeks, use_basics):\n",
    "\n",
    "    print_gpu_utilization()\n",
    "\n",
    "    info, prompt = construct_prompt(ticker, date, n_weeks, use_basics)\n",
    "\n",
    "    # additions:\n",
    "    # save prompt to a file\n",
    "    with open(\"./prompt.txt\", 'w', encoding='utf-8') as file:\n",
    "        file.write(prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors='pt', padding=False\n",
    "    )\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "    print(\"Inputs loaded onto devices.\")\n",
    "\n",
    "    res = model.generate(\n",
    "        **inputs, max_length=4096, do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True, streamer=streamer\n",
    "    )\n",
    "    output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
    "    answer = re.sub(r'.*\\[/INST\\]\\s*', '', output, flags=re.DOTALL)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return info, answer, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "a2fc2f6ab81b44c6af6e4dc194228618",
      "9f414d01bca749e7a0e7bd4e1e7b7157",
      "859c65682ba24ef2814ad29e5dcbec59",
      "a0960e3dd855406fa3ec1b17aa97ce04",
      "6855fc16615d469bb64a770e628f7857",
      "1a07ddf7a54041a29230f7e54baa65ac",
      "4d98b9602e58401e90900ff7f4d5f4c7",
      "623b9e2ed7604aceaef1784ade832a43",
      "3e97eb1e1f084594bdc42d16f8ecb702",
      "2bad6c2304e14cb7ba43b059bed07235",
      "847e34d607c04a13b92388083df79dbe"
     ]
    },
    "id": "YtBwxKM6_FTq",
    "outputId": "f42e473b-5173-4e77-da6b-2ff0e3ed461e"
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'NousResearch/Llama-2-7b-chat-hf',\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    offload_folder=\"offload/\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    'FinGPT/fingpt-forecaster_dow30_llama2-7b_lora',\n",
    "    offload_folder=\"offload/\"\n",
    ")\n",
    "model = model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'NousResearch/Llama-2-7b-chat-hf',\n",
    "    )\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and basic financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. \" \\\n",
    "    \"Your answer format should be as follows:\\n\\n[Positive Developments]:\\n1. ...\\n\\n[Potential Concerns]:\\n1. ...\\n\\n[Prediction & Analysis]\\nPrediction: ...\\nAnalysis: ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbQPxeCRB0mn"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def extract_and_append_prediction_to_file(date, offset, option, ticker):\n",
    "    # Define the pattern to search for\n",
    "    pattern = r\"(Down by \\d+-\\d+%|Up by \\d+-\\d+%)\"\n",
    "    # Find all occurrences of the pattern\n",
    "    matches = re.findall(pattern, answer)\n",
    "    # Open the file in append mode and write the matches\n",
    "    with open(f\"{ticker}_from:{date}_past_{option}:{offset}.txt\", 'a', encoding='utf-8') as file:\n",
    "        for match in matches:\n",
    "            file.write(match + '\\n')\n",
    "            print(\"-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ prediction\")\n",
    "\n",
    "def extract_and_append_reasoning_to_file(output, date, offset, option, ticker):\n",
    "    # Open the file in append mode and write the matches\n",
    "    with open(f\"{ticker}_reasoning_from:{date}_past_{option}:{offset}.txt\", 'a', encoding='utf-8') as file:\n",
    "        file.write(output + '\\n')\n",
    "        file.write('-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\\n')\n",
    "        print(\"-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ output\")\n",
    "\n",
    "# Run the process 10 times\n",
    "for _ in range(10):\n",
    "    ticker = \"TSLA\"\n",
    "    date = \"2024-01-24\" # get_curday()\n",
    "    offset = 1\n",
    "\n",
    "    option = {\n",
    "        \"days\": \"days\",\n",
    "        \"weeks\": \"weeks\"\n",
    "    }\n",
    "    option = \"days\"\n",
    "\n",
    "    text, answer, output = predict(ticker, date, offset, False)\n",
    "    extract_and_append_prediction_to_file(date, offset, option, ticker)\n",
    "    extract_and_append_reasoning_to_file(output, date, offset, option, ticker)\n",
    "    count+=1\n",
    "    print(\"-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_print_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            print(f\"Contents of {filename}:\")\n",
    "            for line in file:\n",
    "                print(line.strip())  # .strip() removes any leading/trailing whitespace, including newline characters\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found.\")\n",
    "\n",
    "# Example usage\n",
    "filename = \"TSLA_from:2024-01-24_past_days:1.txt\"\n",
    "#           TSLA_from:2023-11-9_past_days:3.txt\n",
    "read_and_print_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find key sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a07ddf7a54041a29230f7e54baa65ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bad6c2304e14cb7ba43b059bed07235": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e97eb1e1f084594bdc42d16f8ecb702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4d98b9602e58401e90900ff7f4d5f4c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "623b9e2ed7604aceaef1784ade832a43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6855fc16615d469bb64a770e628f7857": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "847e34d607c04a13b92388083df79dbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "859c65682ba24ef2814ad29e5dcbec59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_623b9e2ed7604aceaef1784ade832a43",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e97eb1e1f084594bdc42d16f8ecb702",
      "value": 2
     }
    },
    "9f414d01bca749e7a0e7bd4e1e7b7157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a07ddf7a54041a29230f7e54baa65ac",
      "placeholder": "​",
      "style": "IPY_MODEL_4d98b9602e58401e90900ff7f4d5f4c7",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "a0960e3dd855406fa3ec1b17aa97ce04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bad6c2304e14cb7ba43b059bed07235",
      "placeholder": "​",
      "style": "IPY_MODEL_847e34d607c04a13b92388083df79dbe",
      "value": " 2/2 [01:07&lt;00:00, 30.92s/it]"
     }
    },
    "a2fc2f6ab81b44c6af6e4dc194228618": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f414d01bca749e7a0e7bd4e1e7b7157",
       "IPY_MODEL_859c65682ba24ef2814ad29e5dcbec59",
       "IPY_MODEL_a0960e3dd855406fa3ec1b17aa97ce04"
      ],
      "layout": "IPY_MODEL_6855fc16615d469bb64a770e628f7857"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
