{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jWq6edktVvVB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets\n",
    "!pip install wandb\n",
    "!pip install torch\n",
    "!pip install Cython\n",
    "\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install -U transformers\n",
    "!pip install peft\n",
    "!pip install -U bitsandbytes\n",
    "!pip install tensorboard\n",
    "!pip install accelerate -U\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6YnI8mFP9tiM"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d5e3cf3f3940348f162aec9eeb34db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "base_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"dohonba/mistral_7b_fingpt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_prompt = \"\"\"\n",
    "# Is this sentence self-promotional? Answer with {no/yes}? \"Building brick by brick, our analysts motto! Pay a visit to our Community\".\n",
    "# \"\"\"\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "#     decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "# print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhUFh_uaXG14",
    "outputId": "526a32e8-6772-4981-a08b-9fa98cee7de0"
   },
   "outputs": [],
   "source": [
    "# Function to classify emotion of a sentence\n",
    "def classify_sentiment(sentence):\n",
    "    eval_prompt = f\"\"\"Context: {sentence}\n",
    "\n",
    "Question: 'What is the sentiment of this sentence? Please choose an answer from {{strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}}.'\n",
    "\"\"\"\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "        decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    answer = decoded_output.split(\"Answer: \", 1)[1]\n",
    "    return answer  # You might need to further process this to extract the emotion\n",
    "\n",
    "def classify_emotion(sentence):\n",
    "    eval_prompt = f\"\"\"Context: {sentence}\n",
    "\n",
    "Question: 'What is the emotion shown in this text? Please choose an answer from {{anger/fear/joy/love/sadness/surprise/neutral}}'.\n",
    "\"\"\"\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "        decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    answer = decoded_output.split(\"Answer: \", 1)[1]\n",
    "    return answer  # You might need to further process this to extract the emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wp5o1ZXYXVy6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_emotion(\"I love it. Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = NewsPlease.from_url('https://finance.yahoo.com/news/alaska-airlines-begun-flying-boeing-150009733.html')\n",
    "\n",
    "# Split the article text into sentences\n",
    "sentences = sent_tokenize(article.maintext)\n",
    "print(\"Sentences in the article: \", len(sentences))\n",
    "\n",
    "# Classify emotion for each sentence with a progress bar\n",
    "emotion_results = []\n",
    "for i, sentence in enumerate(tqdm(sentences, desc=\"Processing Sentences\")):\n",
    "    emotion = classify_sentiment(sentence)\n",
    "    emotion_results.append(emotion)\n",
    "\n",
    "# Do something with the results\n",
    "print(emotion_results)\n",
    "\n",
    "# print(article.maintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Tweets:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  10%|█         | 1/10 [00:03<00:32,  3.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  20%|██        | 2/10 [00:06<00:27,  3.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  30%|███       | 3/10 [00:09<00:22,  3.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  40%|████      | 4/10 [00:13<00:19,  3.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  50%|█████     | 5/10 [00:16<00:16,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  60%|██████    | 6/10 [00:20<00:13,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  70%|███████   | 7/10 [00:23<00:10,  3.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  80%|████████  | 8/10 [00:26<00:06,  3.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  90%|█████████ | 9/10 [00:30<00:03,  3.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets: 100%|██████████| 10/10 [00:33<00:00,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0: mildly negative', '1: strong negative', '2: neutral', '3: strong negative', '4: strong positive', '5: mildly negative', '6: strong negative', '7: neutral', '8: mildly positive', '9: strong positive']\n",
      "['0: neutral', '1: neutral', '2: neutral', '3: neutral', '4: neutral', '5: neutral', '6: surprise', '7: neutral', '8: neutral', '9: neutral']\n",
      "0: Do something original pls\n",
      "1: who's 'special groups'?!\n",
      "2: It will be specific for Nio cars, Tesla did the same years ago as well\n",
      "3: Tesla influencers are waisting their time. Your followers will trend down just as Tesla stock is . If institutional investors are not buying the stock , how will the retail support the stock price ohhhh geee, it’s 194 today . But 3 years ago it was 194. You get it yet ?\n",
      "4: Become the best trader with us:\n",
      "5: Dis aint Nicola tesla, dis nick Rivian bruh.\n",
      "6: what!? what about Tesla's??\n",
      "7: Can The Tesla Cybertruck Power A Travel Trailer For Off-Grid Camping?\n",
      "8: Tesla Adds Beautiful New 3D Maps in Chinese New Year Update - Not a Tesla App\n",
      "9: - Mind blowing potential here. Join us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "# Calculate the cutoff time for the last 30 minutes\n",
    "cutoff_time = datetime.utcnow().replace(tzinfo=pytz.utc) - timedelta(minutes=30)\n",
    "\n",
    "def load_recent_tweets(file_path, cutoff_time):\n",
    "    recent_tweets = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            tweet_time = datetime.fromisoformat(tweet['date'])  # Assuming UTC and removing 'Z'\n",
    "            if tweet_time > cutoff_time:\n",
    "                recent_tweets.append(tweet)  # Assuming text is under 'rawContent'\n",
    "    return recent_tweets\n",
    "\n",
    "def save_processed_tweets(file_path, tweets):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for tweet in tweets:\n",
    "            # Create a new dictionary with only the required fields\n",
    "            filtered_tweet = {\n",
    "                'id': tweet['id'],  # Assuming each tweet has a unique 'id'\n",
    "                'date': tweet['date'],\n",
    "                'cleanContent': tweet.get('cleanContent', ''),\n",
    "                'emotion': tweet.get('emotion', ''),\n",
    "                'sentiment': tweet.get('sentiment', '')\n",
    "            }\n",
    "            json.dump(filtered_tweet, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "tweets = load_recent_tweets('english_tweets.jsonl', cutoff_time)\n",
    "print(len(tweets))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "limit = 10\n",
    "tweets = tweets[:limit]\n",
    "sentiment_results = []\n",
    "emotion_results = []\n",
    "for i, tweet in enumerate(tqdm(tweets, desc=\"Analyzing Tweets\")):\n",
    "    clean_content = tweet.get('cleanContent', '')\n",
    "    \n",
    "    sentiment = classify_sentiment(clean_content)\n",
    "    emotion = classify_emotion(clean_content)\n",
    "    \n",
    "    sentiment_results.append(f\"{i}: \" + sentiment)\n",
    "    emotion_results.append(f\"{i}: \" + emotion)\n",
    "    \n",
    "    tweet['sentiment'] = sentiment\n",
    "    tweet['emotion'] = emotion\n",
    "\n",
    "#Save inference\n",
    "save_processed_tweets('processed_english_tweets.jsonl', tweets)\n",
    "\n",
    "# Print results\n",
    "print(sentiment_results)\n",
    "print(emotion_results)\n",
    "for i, tweet in enumerate(tweets):\n",
    "    print(f\"{i}: \" + tweet.get('cleanContent', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install news-please\n",
    "from newsplease import NewsPlease\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
