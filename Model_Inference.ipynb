{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jWq6edktVvVB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets\n",
    "!pip install wandb\n",
    "!pip install torch\n",
    "!pip install Cython\n",
    "\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install -U transformers\n",
    "!pip install peft\n",
    "!pip install -U bitsandbytes\n",
    "!pip install tensorboard\n",
    "!pip install accelerate -U\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6YnI8mFP9tiM"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d5e3cf3f3940348f162aec9eeb34db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "base_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"dohonba/mistral_7b_fingpt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_prompt = \"\"\"\n",
    "# Is this sentence self-promotional? Answer with {no/yes}? \"Building brick by brick, our analysts motto! Pay a visit to our Community\".\n",
    "# \"\"\"\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "#     decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "# print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhUFh_uaXG14",
    "outputId": "526a32e8-6772-4981-a08b-9fa98cee7de0"
   },
   "outputs": [],
   "source": [
    "# Function to classify emotion of a sentence\n",
    "def classify_sentiment(sentence):\n",
    "    eval_prompt = f\"\"\"Context: {sentence}\n",
    "\n",
    "Question: 'What is the sentiment of this sentence? Please choose an answer from {{strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}}.'\n",
    "\"\"\"\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "        decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    answer = decoded_output.split(\"Answer: \", 1)[1]\n",
    "    return answer  # You might need to further process this to extract the emotion\n",
    "\n",
    "def classify_emotion(sentence):\n",
    "    eval_prompt = f\"\"\"Context: {sentence}\n",
    "\n",
    "Question: 'What is the emotion shown in this text? Please choose an answer from {{anger/fear/joy/love/sadness/surprise/neutral}}'.\n",
    "\"\"\"\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**model_input, max_new_tokens=150)[0]\n",
    "        decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    answer = decoded_output.split(\"Answer: \", 1)[1]\n",
    "    return answer  # You might need to further process this to extract the emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wp5o1ZXYXVy6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_emotion(\"I love it. Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = NewsPlease.from_url('https://finance.yahoo.com/news/alaska-airlines-begun-flying-boeing-150009733.html')\n",
    "\n",
    "# Split the article text into sentences\n",
    "sentences = sent_tokenize(article.maintext)\n",
    "print(\"Sentences in the article: \", len(sentences))\n",
    "\n",
    "# Classify emotion for each sentence with a progress bar\n",
    "emotion_results = []\n",
    "for i, sentence in enumerate(tqdm(sentences, desc=\"Processing Sentences\")):\n",
    "    emotion = classify_sentiment(sentence)\n",
    "    emotion_results.append(emotion)\n",
    "\n",
    "# Do something with the results\n",
    "print(emotion_results)\n",
    "\n",
    "# print(article.maintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Tweets:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  10%|█         | 1/10 [00:03<00:29,  3.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  20%|██        | 2/10 [00:06<00:27,  3.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  30%|███       | 3/10 [00:10<00:24,  3.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  40%|████      | 4/10 [00:14<00:21,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  50%|█████     | 5/10 [00:17<00:17,  3.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  60%|██████    | 6/10 [00:21<00:14,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  70%|███████   | 7/10 [00:24<00:10,  3.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  80%|████████  | 8/10 [00:27<00:06,  3.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets:  90%|█████████ | 9/10 [00:31<00:03,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Analyzing Tweets: 100%|██████████| 10/10 [00:35<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0: strong positive', '1: moderately positive', '2: mildly positive', '3: moderately positive', '4: strong negative', '5: moderately positive', '6: neutral', '7: moderately positive', '8: moderately positive', '9: mildly positive']\n",
      "['0: neutral', '1: neutral', '2: neutral', '3: neutral', '4: anger', '5: neutral', '6: neutral', '7: neutral', '8: neutral', '9: neutral']\n",
      "0: If you invested in on that day, you’d currently be up over 8,800%\n",
      "1: Top analyst price target for next week..\n",
      "2: “.. functions better when he has a woman he loves telling him when he is being an idiot. Like all of us!” Printed &amp; framed!\n",
      "3: Nvdidia bull flag! The bull flags I predicted / pointed out yesterday for SMCI and Tesla are playing out beautifully BTW\n",
      "4: I still cannot believe Sony stole and patented MY INVENTION I created when I was a kid. So far two of my inventions have been stolen and other people are profiting off. Doing me dirty like Tesla and I was only in Elementary School\n",
      "5: The time has come for to buy and\n",
      "6: Basically what Tesla already has in it's operating markets. You get to try a Tesla and book one.\n",
      "7: Where BYD out smart Tesla to become King of EVs via |\n",
      "8: SUMANDO TSLA SI SUBE AL WMA 21 HERMOSA\n",
      "9: looking to cover in 190 or below let’s see\n",
      "10: tesla, the man famous for… being mad at his boss?\n",
      "11: CURVING A FAT ONE already up .\n",
      "12: People get caught up in them being battery-electric SUVs, but they have wildly different styling/product messaging targeting different tastes/buyers. Your question is akin to asking why anyone would pick BMW X6 over Jeep Grand Wagoneer. It makes no sense.\n",
      "13: there is one specific intersection in our quiet neighborhood where i have to turn left and if i had a dollar for every time ive been waiting for a tesla to pass so i can cross only for them to right turn without signaling into the street im waiting on…\n",
      "14: The time has come for to buy and\n",
      "15: Tesla Daily gap fill - broadening target :\n",
      "16: Morgan stanley raises hilsings target to due to AI growth. 200 $ralli rali\n",
      "17: It’s not an easy task to put forward the chart analysis and also execute it!!! I sticked to my plan and here are the rewards!! Especially with reaping + profit within a week!! Nailed the puts for 200% gain earlier and now the reversal gave 120% overall gain on my huge amount!! Locked 50% here and riding the remaining towards the target by adding hedge here! Patience pays off!! Please do like and retweet so it can help me to reach to larger audience. Appreciate your efforts!!\n",
      "18: Who in the world puts down on a lease\n",
      "19: Tesla app which includes charging data\n",
      "20: Explore the possibilities with Spark Energy Minerals Inc. ( ) on the Canadian Securities Exchange. With a market cap of over .6M, this company presents exciting opportunities in the energy sector.\n",
      "21: Our Performance Challenge T2K: Trading Days: 106 Net % Gain: 114.9% Closed Trades: 49 Green Trades: 40 Red Trades: 6 Current Exposure: 7%. and more\n",
      "22: No experience of Rivian.\n",
      "23: A new Agent Tesla variant. Hot wallet hacks. DevSecOps and AI. Notes on the labor market. Two threats in a hybrid war: Fancy Bear and NoName057(16). New Agent Tesla variant is out. Lost credentials and crypto wallet hacks. Tension between DevSecOps and...\n",
      "24: - Mind blowing potential here. Join us\n",
      "25: BULENOX 87% OFF! CODE: ATARI87\n",
      "26: TSLA Feb16 Batman: 150/175/210/235 Iron Condor + 180/205 Strangle In Feb08 11:56am; out Feb09 9:31am for\n",
      "27: Gm already out of an21% gain on calls and a 15% gain on puts, thinking about buying someone lunch today now. How’s your morning going?\n",
      "28: 2\n",
      "29: Gm already out of a 21% gain on calls and a 15% gain on puts, thinking about buying someone lunch today now. How’s your morning going?\n",
      "30: Tape bullish at calls n chill\n",
      "31: It’s not that Americans don’t want EVs. numbers alone are proof. and cannot make EVs profitably within their existing business model. They refuse to change their business model to conform to the state change and are opting instead to compel consumers to buy ICE vehicles all the while disparaging EVs. They are the last buggy whip makers who were failing prior to the EV revolution. please fact check\n",
      "32: How about in your Tesla?\n",
      "33: They won't. That's just not Elon's way.\n",
      "34: name Put Respect On It I told you 2 years ago I was the best chartist alive\n",
      "35: If OEMs weren’t in the dark ages this wouldn’t be a problem. But a Tesla, install pin to drive, solved.\n",
      "36: Morgan stanley raises hilsings target to due to AI growth. 200 $ralli rali\n",
      "37: Another week went pretty well Big or small ,Gain is what it matters Congratulations to all the members of our ChatRoom\n",
      "38: Someone buying out the lease. Rivians can be bought out at the end. This is the default down payment for Tesla leases for apples-to-apples.\n",
      "39: I think tesla takes the cake\n",
      "40: Omg my Samsung phone alerted me to a recall! Not really, just a software update.. but if It was a tesla front page news on a recall.\n",
      "41: This now means that negative publicity by companies receiving ad dollars from Tesla cannot be about software recalls any longer.\n",
      "42: What Is Going On With Tesla's Dojo? - CleanTechnica\n",
      "43: Gaotu: Live-Streaming Business, Financial Outlook Are Key Investment Considerations (NYSE: )\n",
      "44: Week 5 Model Y Top 10 Pure EV Contenders: Important observations: 1. Tesla Model Y sales were slightly below weekly trend, but still captured a major share of the top 10 (54.85%); 2. ES6 and EC6 performed best this week of January, with 1,270 (9.07%) and 482 (3.44%) units sold, respectively; 3. and IM are neck and neck at 4th and 5th, as the XPeng G6 jumped to 797 (5.69%) units sold; 4. Notable performers are AITO M5 and Q4 e-tron, both achieving monthly highs;\n",
      "45: Engineering progress. We used to have a lot more insight. I would love to hear more from leaders within Tesla now that Musk is stuck at the border.\n",
      "46: \n",
      "47: Bought TSLA instead of NIO. Also bought DOCN few months back looks really good.\n",
      "48: The Tesla Roadster’s battery system consists of 6,831 individual Li-ion (lithium-ion) cells, in the back of the vehicle, roughly the size of a storage trunk, and weighing about 900 pounds.\n",
      "49: Next week is going to be a battle between bulls and bears\n",
      "50: 20 . . ...\n",
      "51: I wonder if he got so used to his businesses largely funded by US tax payer stuff (Tesla carbon wotsits, Spacex ISS journeys, satellite internet, Vegas car tunnel etc ) he became accustomed to thinking everyone in the entire world owed him payment.\n",
      "52: nice hammer candle forming on W chart. Support off 61.8%fib proving to be clutch\n",
      "53: Top Buzzing Cryptos: CROWD Top 3 Buzzing Cryptos: MP | Check out sentiment and other crypto stats at\n",
      "54: Ticker : Closed : Played S3 to S4 and closed the week with the best way.\n",
      "55: rises after Putin said Elon musk is a smart guy\n",
      "56: That would all be fine if he didn't constantly compare FUV with Tesla and hence impicitly suggesting to his audience \"look this is the next Tesla\". That was the crime. Why? Because it was highly salacious attracting loads of views. Look it's simple: if you make videos about investing, which are inherently boring, you need to provide some perceived value to the audience to keep them 'entertained'. That value is implying to the audience they can make money if they listen to you.\n",
      "57: \n",
      "58: I get it, but most people remove them and put all-weather in. And Tesla carpet is so good, why bother protecting it with another carpet floor mat?\n",
      "59: Tesla EVs get a big Phone Key boost, but only if you're an iPhone user\n",
      "60: Wedbush reiterates Tesla as outperform Wedbush said it’s sticking with its outperform rating on the stock but that’s concerned about the negative narrative surrounding the stock. “We believe the Tesla narrative is as negative as we have seen in the last few years with Musk and Tesla getting attacked by the bears from all directions.”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "# Calculate the cutoff time for the last 30 minutes\n",
    "cutoff_time = datetime.utcnow().replace(tzinfo=pytz.utc) - timedelta(minutes=30)\n",
    "\n",
    "def load_recent_tweets(file_path, cutoff_time):\n",
    "    recent_tweets = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            tweet_time = datetime.fromisoformat(tweet['date'])  # Assuming UTC and removing 'Z'\n",
    "            if tweet_time > cutoff_time:\n",
    "                recent_tweets.append(tweet)  # Assuming text is under 'rawContent'\n",
    "    return recent_tweets\n",
    "\n",
    "def save_processed_tweets(file_path, tweets):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for tweet in tweets:\n",
    "            # Create a new dictionary with only the required fields\n",
    "            filtered_tweet = {\n",
    "                'id': tweet['id'],  # Assuming each tweet has a unique 'id'\n",
    "                'date': tweet['date'],\n",
    "                'cleanContent': tweet.get('cleanContent', ''),\n",
    "                'emotion': tweet.get('emotion', ''),\n",
    "                'sentiment': tweet.get('sentiment', '')\n",
    "            }\n",
    "            json.dump(filtered_tweet, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "tweets = load_recent_tweets('english_tweets.jsonl', cutoff_time)\n",
    "print(len(tweets))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "limit = 10\n",
    "sentiment_results = []\n",
    "emotion_results = []\n",
    "for i, tweet in enumerate(tqdm(tweets[:limit], desc=\"Analyzing Tweets\")):\n",
    "    clean_content = tweet.get('cleanContent', '')\n",
    "    \n",
    "    sentiment = classify_sentiment(clean_content)\n",
    "    emotion = classify_emotion(clean_content)\n",
    "    \n",
    "    sentiment_results.append(f\"{i}: \" + sentiment)\n",
    "    emotion_results.append(f\"{i}: \" + emotion)\n",
    "    \n",
    "    tweet['sentiment'] = sentiment\n",
    "    tweet['emotion'] = emotion\n",
    "\n",
    "#Save inference\n",
    "save_processed_tweets('processed_english_tweets.jsonl', tweets)\n",
    "\n",
    "# Print results\n",
    "print(sentiment_results)\n",
    "print(emotion_results)\n",
    "for i, tweet in enumerate(tweets):\n",
    "    print(f\"{i}: \" + tweet.get('cleanContent', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install news-please\n",
    "from newsplease import NewsPlease\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
